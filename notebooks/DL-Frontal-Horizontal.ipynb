{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "executionInfo": {
     "elapsed": 6048,
     "status": "ok",
     "timestamp": 1696081183440,
     "user": {
      "displayName": "Ardhani Dzaky Ralfiano",
      "userId": "00254749370135514461"
     },
     "user_tz": -420
    },
    "id": "ZfdIIzegqPOp",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1696081183441,
     "user": {
      "displayName": "Ardhani Dzaky Ralfiano",
      "userId": "00254749370135514461"
     },
     "user_tz": -420
    },
    "id": "LGvo_oyMqtzw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "PyTorch version: 2.1.0\n",
      "torchvision version: 0.16.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "cv2.setRNGSeed(42)\n",
    "\n",
    "ROOT = '../../dataset/v1'\n",
    "\n",
    "MODEL_CKP = {\n",
    "    'effnet': '../../pretrained_ckp/efficientnet_b0.pth'\n",
    "    , 'shufflenet': '../../pretrained_ckp/shufflenet_v2.pth'\n",
    "    , 'mobilenet': '../../pretrained_ckp/mobilenet_v2.pth'\n",
    "}\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "print(device)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"torchvision version:\", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop Face Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_face(image_path, target_width=300, target_height=400):\n",
    "    \"\"\"\n",
    "    Crop and resize the detected face in an image.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): The path to the input image file.\n",
    "    - target_width (int, optional): The desired width of the output face image (default is 300 pixels).\n",
    "    - target_height (int, optional): The desired height of the output face image (default is 400 pixels).\n",
    "\n",
    "    Returns:\n",
    "    - resized_rgb (numpy.ndarray): The cropped and resized face region as a NumPy array in Gray color format.\n",
    "    - resized_gray (numpy.ndarray): The cropped and resized face region as a NumPy array in Gray color format.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the image cannot be loaded from the given path or if no faces are detected in the image.\n",
    "\n",
    "    This function takes an image file, detects the face in the image, and resizes it to match the specified target \n",
    "    width and height while maintaining the aspect ratio. The result is returned as a NumPy array in RGB and Gray color format.\n",
    "\n",
    "    Example usage:\n",
    "    >>> original_image, gray_iamge = crop_face(\"your_image.jpg\")\n",
    "    \"\"\"\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Search for faces on the image.\n",
    "    face_cascade = cv2.CascadeClassifier(\"../../haarcascade_frontalface_default.xml\")\n",
    "    faces = face_cascade.detectMultiScale(image, scaleFactor=1.1, minNeighbors=5)\n",
    "    x, y, w, h = faces[0]\n",
    "\n",
    "    # Adjust height, so it will create a 3:4 (width:height) ratio.\n",
    "    exp_ratio = 3 / 4\n",
    "    h = int(w / exp_ratio)\n",
    "\n",
    "    # Adjust y, as a pre-caution if it \n",
    "    # being cropped below the forehead.\n",
    "    y -= int((image.shape[0] / target_height) * 35)\n",
    "    \n",
    "    # Add padding for the height, as a pre-caution\n",
    "    # if it being cropped below the forehead.\n",
    "    if y + h > image.shape[0]:\n",
    "        minus_y = y + h - image.shape[0]\n",
    "        y -= minus_y\n",
    "\n",
    "    image_cropped = image_rb[y:y+h, x:x+w]\n",
    "    image_cropped_resized = cv2.resize(image_cropped, (target_width, target_height))\n",
    "    \n",
    "    resized_rgb = image_cropped_resized\n",
    "    resized_gray = cv2.cvtColor(resized_rgb, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    return resized_rgb, resized_gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch dataset for working with image data and labels stored in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (pandas.DataFrame): The DataFrame containing image file paths and corresponding labels.\n",
    "    - root (str): The root directory where the image files are located.\n",
    "    - transform (callable, optional): A function/transform to apply to the images (e.g., data augmentation).\n",
    "\n",
    "    Attributes:\n",
    "    - dataframe (pandas.DataFrame): The input DataFrame containing image file paths and labels.\n",
    "    - root (str): The root directory where image files are stored.\n",
    "    - transform (callable, optional): A function/transform to be applied to the images.\n",
    "\n",
    "    Methods:\n",
    "    - __len__(): Returns the number of samples in the dataset.\n",
    "    - __getitem__(idx): Returns the image and label for the specified index.\n",
    "\n",
    "    This dataset class is designed to work with image data stored in a DataFrame, where each row contains\n",
    "    a file path to an image and its corresponding label. It allows for data loading, cropping, and optional\n",
    "    data transformation using PyTorch's data loading utilities. \n",
    "\n",
    "    Example usage:\n",
    "    >>> dataset = CustomDataset(dataframe, root_dir, transform=transforms.Compose([transforms.Resize(256), transforms.ToTensor()]))\n",
    "    >>> dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    >>> for images, labels in dataloader:\n",
    "    >>>     # Process the batch of images and labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self\n",
    "        , dataframe\n",
    "        , root\n",
    "        , transform\n",
    "        , target_width\n",
    "        , target_height\n",
    "    ):\n",
    "        self.dataframe = dataframe\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "        self.target_width = target_width\n",
    "        self.target_height = target_height\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, self.dataframe.iloc[idx, 0])\n",
    "        image, _ = crop_face(img_path, target_width=self.target_width, target_height=self.target_height)\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "        label = int(self.dataframe.iloc[idx, 1])\n",
    "\n",
    "        if self.transform: \n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FKGTask Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FKGTask:\n",
    "    \"\"\"\n",
    "    Designed to handle and prepare data for classification tasks. \n",
    "    It provides methods to encode categorical labels, split data into training and validation sets, \n",
    "    and retrieve the encoded data, making it useful for machine learning tasks.\n",
    "\n",
    "    Parameters:\n",
    "    - face_side (str): The column name representing the face side in the dataset.\n",
    "    - subtask (str): The column name representing the subtask or label in the dataset.\n",
    "    - dataframe (pd.DataFrame): The Pandas DataFrame containing the encoded data.\n",
    "\n",
    "    Attributes:\n",
    "    - face_side (str): The column name representing the face side in the dataset.\n",
    "    - subtask (str): The column name representing the subtask or label in the dataset.\n",
    "    - dataframe (pd.DataFrame): The Pandas DataFrame containing the encoded data.\n",
    "    - encoding_dict (dict): A dictionary that maps the original class labels to their encoded values.\n",
    "    - X (pd.Series): The feature data (independent variable).\n",
    "    - y (pd.Series): The target data (dependent variable).\n",
    "\n",
    "    Methods:\n",
    "    - __init__(self, face_side, subtask, data): Initializes an instance of the FKGTask class.\n",
    "    - _get_dataframe(self, data, subtask): Encodes the categorical labels in the dataset and returns the encoded DataFrame and encoding dictionary.\n",
    "    - _get_x_and_y(self): Extracts the feature and target data from the DataFrame.\n",
    "    - get_train_test_split(self): Splits the data into training and validation sets and returns them as DataFrames.\n",
    "\n",
    "    Example Usage:\n",
    "    >>> fkg_task = FKGTask(face_side=`face_side`, subtask=`subtask`, data=my_data)\n",
    "    >>> train_data, val_data = fkg_task.get_train_test_split()\n",
    "    \n",
    "    >>>  # Train a machine learning model using the train_data\n",
    "    >>>  # Validate the model using the val_data\n",
    "\n",
    "    Note:\n",
    "    - This class is designed to work with Pandas DataFrames and assumes that the input data contains columns corresponding to the specified `face_side` and `subtask`.\n",
    "    - It uses ordinal encoding to convert categorical labels into numerical values.\n",
    "    - The class provides a convenient way to split the data into training and validation sets, maintaining the stratified distribution of the target variable.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, face_side, subtask, data):\n",
    "        self.face_side = face_side\n",
    "        self.subtask = subtask\n",
    "        self.dataframe, self.encoding_dict = self._get_dataframe(data, self.subtask)\n",
    "        self.X, self.y = self._get_x_and_y()\n",
    "\n",
    "    def _get_dataframe(self, data, subtask):\n",
    "        encoder = OrdinalEncoder()\n",
    "        unique_values = data[subtask][subtask].unique().reshape(-1, 1)\n",
    "        encoder.fit(unique_values)\n",
    "\n",
    "        before_val = data[subtask][subtask]\n",
    "        data[subtask][subtask] = encoder.transform(data[subtask][subtask].values.reshape(-1, 1))\n",
    "        encoding_dict = {original_class: encoded_value for original_class, encoded_value in zip(data[subtask][subtask], before_val)}\n",
    "        encoding_dict = {v: k for k, v in encoding_dict.items()}\n",
    "        \n",
    "        return data[subtask], encoding_dict\n",
    "\n",
    "    def _get_x_and_y(self):\n",
    "        X = self.dataframe[self.face_side]\n",
    "        y = self.dataframe[self.subtask]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def get_train_test_split(self):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            self.X, self.y\n",
    "            , stratify=self.y\n",
    "            , test_size=0.2\n",
    "            , random_state=42\n",
    "        )\n",
    "\n",
    "        return pd.concat([X_train, y_train], axis=1), pd.concat([X_val, y_val], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_checkpoint(model, ckp_path):\n",
    "    \"\"\"\n",
    "    Load a PyTorch model from a checkpoint file.\n",
    "    This function loads a pre-trained or saved PyTorch \n",
    "    model from a checkpoint file and returns the loaded model.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The PyTorch model to be loaded.\n",
    "    - ckp_path (str): The path to the checkpoint file.\n",
    "\n",
    "    Returns:\n",
    "    - loaded_model (torch.nn.Module): The model loaded from the checkpoint.\n",
    "\n",
    "    Example Usage:\n",
    "    >>> # Load a pre-trained model from a checkpoint file\n",
    "    >>> model = load_model_checkpoint(models.resnet18(pretrained=False), 'model_checkpoint.pth')\n",
    "\n",
    "    Note:\n",
    "    - Ensure that the model architecture in the checkpoint file matches the provided `model` argument.\n",
    "    \"\"\"\n",
    "    temp_model = model\n",
    "    checkpoint = torch.load(ckp_path)\n",
    "    temp_model.load_state_dict(checkpoint)\n",
    "\n",
    "    return temp_model\n",
    "\n",
    "def get_models():\n",
    "    \"\"\"\n",
    "    Get a dictionary of pre-trained models.\n",
    "    This function loads and returns a dictionary of pre-trained PyTorch models, \n",
    "    including EfficientNet B0, ShuffleNet V2, and MobileNet V2.\n",
    "\n",
    "    Returns:\n",
    "    - model_dict (dict): A dictionary with model names as keys and the corresponding pre-trained models as values.\n",
    "\n",
    "    Example Usage:\n",
    "    >>> # Get a dictionary of pre-trained models\n",
    "    >>> models_dict = get_models()\n",
    "    >>> effnet_model = models_dict['effnet']\n",
    "    >>> shufflenet_model = models_dict['shufflenet']\n",
    "    >>> mobilenet_model = models_dict['mobilenet']\n",
    "\n",
    "    Note:\n",
    "    - Make sure to import the necessary PyTorch model modules from `torchvision.models`.\n",
    "    \"\"\"\n",
    "    efficientnet_b0 = load_model_checkpoint(models.efficientnet_b0(pretrained=False), MODEL_CKP['effnet'])  \n",
    "    shufflenet = load_model_checkpoint(models.shufflenet_v2_x1_0(pretrained=False), MODEL_CKP['shufflenet'])\n",
    "    mobilenet_v2 = load_model_checkpoint(models.mobilenet_v2(pretrained=False), MODEL_CKP['mobilenet'])\n",
    "\n",
    "    model_dict = {}\n",
    "    model_dict['effnet'] = efficientnet_b0\n",
    "    model_dict['shufflenet'] = shufflenet\n",
    "    model_dict['mobilenet'] = mobilenet_v2\n",
    "\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Module Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FKGLightningModuleV1(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    FKGLightningModuleV1 is a PyTorch Lightning Module for image classification tasks. It allows training and validation of\n",
    "    models with different architectures, such as EfficientNet, MobileNet, and ShuffleNet for classification problems.\n",
    "\n",
    "    Parameters:\n",
    "    - num_classes (int): The number of classes for the classification task.\n",
    "    - model_name (str): The name of the model architecture to use from the model_dict.\n",
    "    - model_dict (dict): A dictionary containing pre-trained PyTorch models.\n",
    "\n",
    "    Attributes:\n",
    "    - num_classes (int): The number of classes for the classification task.\n",
    "    - model_name (str): The name of the model architecture being used.\n",
    "    - model_dict (dict): A dictionary containing pre-trained PyTorch models.\n",
    "    - model (nn.Module): The neural network model loaded from model_dict.\n",
    "    - criterion (nn.Module): The loss function, Cross-Entropy Loss, used for training.\n",
    "\n",
    "    Methods:\n",
    "    - _load_model(self): Loads the specified model architecture and adjusts it for the given task.\n",
    "    - forward(self, x): Performs forward pass through the model.\n",
    "    - configure_optimizers(self): Configures the optimizer for training.\n",
    "    - training_step(self, batch, batch_idx): Defines a training step, including forward and loss calculation.\n",
    "    - validation_step(self, batch, batch_idx): Defines a validation step, including forward and loss calculation.\n",
    "\n",
    "    Example Usage:\n",
    "    >>> # Create an instance of FKGLightningModuleV1\n",
    "    >>> model = FKGLightningModuleV1(num_classes=10, model_name='effnet', model_dict=model_dict)\n",
    "\n",
    "    >>> # Configure Lightning Trainer and train the model\n",
    "    >>> trainer = pl.Trainer(gpus=1)\n",
    "    >>> trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "    Note:\n",
    "    - This class is designed to work with PyTorch Lightning for efficient training and validation of image classification models.\n",
    "    - It supports various model architectures, and you need to provide a `model_dict` containing pre-trained models.\n",
    "    - Make sure to customize the model architecture for your specific classification task by modifying the `_load_model` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self\n",
    "        , num_classes\n",
    "        , model_name\n",
    "        , model_dict\n",
    "        , kernel_size=(3, 3)\n",
    "        , change_kernel_size=False\n",
    "    ):\n",
    "        super(FKGLightningModuleV1, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.model_name = model_name\n",
    "        self.model_dict = model_dict\n",
    "\n",
    "        self.training_loss = []\n",
    "        self.training_loss_outputs = []\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.change_kernel_size = change_kernel_size\n",
    "        \n",
    "        self.model = self._load_model()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _change_kernel_size(self, model, model_type, kernel_size):\n",
    "        new_kernel_size = kernel_size\n",
    "        modified_first_conv = False\n",
    "    \n",
    "        if model_type == 'shufflenet':\n",
    "            for layer in model.children():\n",
    "                if isinstance(layer, nn.Sequential):\n",
    "                    for child_layer in layer:\n",
    "                        if isinstance(child_layer, nn.Conv2d):\n",
    "                            if not modified_first_conv:     \n",
    "                                # Modify the kernel_size for the first Conv2d layer in the first Sequential block\n",
    "                                child_layer.kernel_size = new_kernel_size\n",
    "                                modified_first_conv = True\n",
    "        \n",
    "        if model_type == 'effnet':\n",
    "            for layer in model.children():\n",
    "                if isinstance(layer, nn.Sequential):\n",
    "                    for child_layer in layer:\n",
    "                        for grandchild_layer in child_layer:\n",
    "                            if isinstance(grandchild_layer, nn.Conv2d):\n",
    "                                if not modified_first_conv:     \n",
    "                                    # Modify the kernel_size for the first Conv2d layer in the first Sequential block\n",
    "                                    grandchild_layer.kernel_size = new_kernel_size\n",
    "                                    modified_first_conv = True\n",
    "        \n",
    "        if model_type == 'mobilenet':\n",
    "            for layer in model.children():\n",
    "                if isinstance(layer, nn.Sequential):\n",
    "                    for grandchild_layer in layer[0]:\n",
    "                        if isinstance(grandchild_layer, nn.Conv2d):\n",
    "                            if not modified_first_conv:     \n",
    "                                # Modify the kernel_size for the first Conv2d layer in the first Sequential block\n",
    "                                grandchild_layer.kernel_size = new_kernel_size\n",
    "                                modified_first_conv = True\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def _load_model(self):\n",
    "        temp_model = self.model_dict[self.model_name]\n",
    "        \n",
    "        if self.model_name in ['effnet', 'mobilenet']:\n",
    "            num_feat = temp_model.classifier[-1].in_features\n",
    "            \n",
    "        if self.model_name in ['shufflenet', 'resnet']:\n",
    "            num_feat = temp_model.fc.in_features\n",
    "\n",
    "        model = nn.Sequential(*list(temp_model.children())[:-1])\n",
    "        model.add_module('global_avg_pool', nn.AdaptiveAvgPool2d(1))\n",
    "        model.add_module('flatten', nn.Flatten())\n",
    "        model.add_module('fc', nn.Linear(num_feat, self.num_classes))\n",
    "\n",
    "        if self.change_kernel_size:\n",
    "            model = self._change_kernel_size(model, self.model_name, self.kernel_size)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)   \n",
    "\n",
    "        self.training_loss_outputs.append(loss)\n",
    "        return loss\n",
    "\n",
    "    # https://github.com/Lightning-AI/lightning/discussions/17182\n",
    "    def on_train_epoch_end(self):\n",
    "        epoch_average = torch.stack(self.training_loss_outputs).mean()\n",
    "        self.training_loss.append(epoch_average)\n",
    "        self.training_loss_outputs.clear()\n",
    "   \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        val_loss = self.criterion(outputs, labels)\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nama</th>\n",
       "      <th>tampak_depan_root</th>\n",
       "      <th>full_depan</th>\n",
       "      <th>tampak_samping_root</th>\n",
       "      <th>full_samping</th>\n",
       "      <th>tampak_senyum_root</th>\n",
       "      <th>full_senyum</th>\n",
       "      <th>tipe</th>\n",
       "      <th>simetris</th>\n",
       "      <th>seimbang</th>\n",
       "      <th>transversal</th>\n",
       "      <th>profil</th>\n",
       "      <th>nasolabial</th>\n",
       "      <th>mentolabial</th>\n",
       "      <th>segaris</th>\n",
       "      <th>bukal</th>\n",
       "      <th>kurva</th>\n",
       "      <th>garis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aisyahi</td>\n",
       "      <td>tampak_depan</td>\n",
       "      <td>tampak_depan/aisyahi.jpg</td>\n",
       "      <td>tampak_samping</td>\n",
       "      <td>tampak_samping/aisyahi.jpg</td>\n",
       "      <td>tampak_senyum</td>\n",
       "      <td>tampak_senyum/aisyahi.jpg</td>\n",
       "      <td>doliko</td>\n",
       "      <td>asimetris</td>\n",
       "      <td>tidak</td>\n",
       "      <td>tidak</td>\n",
       "      <td>cekung</td>\n",
       "      <td>normal</td>\n",
       "      <td>lebar</td>\n",
       "      <td>ya</td>\n",
       "      <td>lebar</td>\n",
       "      <td>datar</td>\n",
       "      <td>tinggi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alfira</td>\n",
       "      <td>tampak_depan</td>\n",
       "      <td>tampak_depan/alfira.jpg</td>\n",
       "      <td>tampak_samping</td>\n",
       "      <td>tampak_samping/alfira.jpg</td>\n",
       "      <td>tampak_senyum</td>\n",
       "      <td>tampak_senyum/alfira.jpg</td>\n",
       "      <td>doliko</td>\n",
       "      <td>simetris</td>\n",
       "      <td>ya</td>\n",
       "      <td>ya</td>\n",
       "      <td>cembung</td>\n",
       "      <td>tajam</td>\n",
       "      <td>tumpul</td>\n",
       "      <td>ya</td>\n",
       "      <td>normal</td>\n",
       "      <td>konsonan</td>\n",
       "      <td>rendah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>despasya</td>\n",
       "      <td>tampak_depan</td>\n",
       "      <td>tampak_depan/despasya.jpg</td>\n",
       "      <td>tampak_samping</td>\n",
       "      <td>tampak_samping/despasya.jpg</td>\n",
       "      <td>tampak_senyum</td>\n",
       "      <td>tampak_senyum/despasya.jpg</td>\n",
       "      <td>doliko</td>\n",
       "      <td>simetris</td>\n",
       "      <td>ya</td>\n",
       "      <td>tidak</td>\n",
       "      <td>cembung</td>\n",
       "      <td>tajam</td>\n",
       "      <td>tumpul</td>\n",
       "      <td>ya</td>\n",
       "      <td>lebar</td>\n",
       "      <td>datar</td>\n",
       "      <td>sedang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>galuh</td>\n",
       "      <td>tampak_depan</td>\n",
       "      <td>tampak_depan/galuh.jpg</td>\n",
       "      <td>tampak_samping</td>\n",
       "      <td>tampak_samping/galuh.jpg</td>\n",
       "      <td>tampak_senyum</td>\n",
       "      <td>tampak_senyum/galuh.jpg</td>\n",
       "      <td>doliko</td>\n",
       "      <td>simetris</td>\n",
       "      <td>ya</td>\n",
       "      <td>ya</td>\n",
       "      <td>cekung</td>\n",
       "      <td>tajam</td>\n",
       "      <td>tumpul</td>\n",
       "      <td>ya</td>\n",
       "      <td>lebar</td>\n",
       "      <td>konsonan</td>\n",
       "      <td>tinggi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "      <td>tampak_depan</td>\n",
       "      <td>tampak_depan/happy.jpg</td>\n",
       "      <td>tampak_samping</td>\n",
       "      <td>tampak_samping/happy.jpg</td>\n",
       "      <td>tampak_senyum</td>\n",
       "      <td>tampak_senyum/happy.jpg</td>\n",
       "      <td>brachy</td>\n",
       "      <td>simetris</td>\n",
       "      <td>ya</td>\n",
       "      <td>tidak</td>\n",
       "      <td>cembung</td>\n",
       "      <td>normal</td>\n",
       "      <td>tumpul</td>\n",
       "      <td>tidak</td>\n",
       "      <td>normal</td>\n",
       "      <td>datar</td>\n",
       "      <td>rendah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nama tampak_depan_root                 full_depan tampak_samping_root  \\\n",
       "0   aisyahi      tampak_depan   tampak_depan/aisyahi.jpg      tampak_samping   \n",
       "1    alfira      tampak_depan    tampak_depan/alfira.jpg      tampak_samping   \n",
       "2  despasya      tampak_depan  tampak_depan/despasya.jpg      tampak_samping   \n",
       "3     galuh      tampak_depan     tampak_depan/galuh.jpg      tampak_samping   \n",
       "4     happy      tampak_depan     tampak_depan/happy.jpg      tampak_samping   \n",
       "\n",
       "                  full_samping tampak_senyum_root                 full_senyum  \\\n",
       "0   tampak_samping/aisyahi.jpg      tampak_senyum   tampak_senyum/aisyahi.jpg   \n",
       "1    tampak_samping/alfira.jpg      tampak_senyum    tampak_senyum/alfira.jpg   \n",
       "2  tampak_samping/despasya.jpg      tampak_senyum  tampak_senyum/despasya.jpg   \n",
       "3     tampak_samping/galuh.jpg      tampak_senyum     tampak_senyum/galuh.jpg   \n",
       "4     tampak_samping/happy.jpg      tampak_senyum     tampak_senyum/happy.jpg   \n",
       "\n",
       "     tipe   simetris seimbang transversal   profil nasolabial mentolabial  \\\n",
       "0  doliko  asimetris    tidak       tidak   cekung     normal       lebar   \n",
       "1  doliko   simetris       ya          ya  cembung      tajam      tumpul   \n",
       "2  doliko   simetris       ya       tidak  cembung      tajam      tumpul   \n",
       "3  doliko   simetris       ya          ya   cekung      tajam      tumpul   \n",
       "4  brachy   simetris       ya       tidak  cembung     normal      tumpul   \n",
       "\n",
       "  segaris   bukal     kurva   garis  \n",
       "0      ya   lebar     datar  tinggi  \n",
       "1      ya  normal  konsonan  rendah  \n",
       "2      ya   lebar     datar  sedang  \n",
       "3      ya   lebar  konsonan  tinggi  \n",
       "4   tidak  normal     datar  rendah  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('../../dataset/df_cls.xlsx')\n",
    "df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 942,
     "status": "ok",
     "timestamp": 1696081309593,
     "user": {
      "displayName": "Ardhani Dzaky Ralfiano",
      "userId": "00254749370135514461"
     },
     "user_tz": -420
    },
    "id": "7Y1OYgMRMkS0"
   },
   "outputs": [],
   "source": [
    "TASKS = [\n",
    "    'tipe', 'simetris', 'seimbang', 'transversal'\n",
    "    , 'profil', 'nasolabial', 'mentolabial', 'segaris'\n",
    "    , 'bukal', 'kurva', 'garis'\n",
    "]\n",
    "df_tasks_dict = {}\n",
    "\n",
    "for task in TASKS:\n",
    "    if task in ['tipe', 'seimbang', 'simetris', 'transversal']:\n",
    "        df_tasks_dict[task] = df[['full_depan', task]]\n",
    "    elif task in ['profil', 'nasolabial', 'mentolabial']:\n",
    "        df_tasks_dict[task] = df[['full_samping', task]]\n",
    "    else: df_tasks_dict[task] = df[['full_senyum', task]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_dict = get_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- PERMUTATION ----- #\n",
    "mod_strings = ['effnet', 'mobilenet', 'shufflenet']\n",
    "subtasks = ['tipe', 'simetris', 'transversal', 'seimbang']\n",
    "resolutions = [[400, 300], [600, 450], [800, 600]]\n",
    "kernel_size = [(3, 3), (4, 3)]\n",
    "# ----------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trial - Edit here\n",
    "epoch_used = 10\n",
    "max_epoch_used = epoch_used + 1\n",
    "max_epoch_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_data_transform(desired_size):\n",
    "    data_transform = T.Compose([\n",
    "        T.Resize(desired_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Use the ImageNet mean and std\n",
    "    ])\n",
    "\n",
    "    return data_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(800, 600), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "data_transform = return_data_transform(tuple(resolutions[2]))\n",
    "data_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change here\n",
    "subtask = 'transversal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/40d0w36j79z901rcfq942hhh0000gn/T/ipykernel_2590/447050079.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[subtask][subtask] = encoder.transform(data[subtask][subtask].values.reshape(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "task = FKGTask('full_depan', subtask, df_tasks_dict.copy())\n",
    "train, val = task.get_train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tidak': 0.0, 'ya': 1.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.encoding_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe related\n",
    "df_model = []\n",
    "df_resolutions = []\n",
    "df_kernel = []\n",
    "\n",
    "df_f1 = []\n",
    "df_recall = []\n",
    "df_precision = []\n",
    "df_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model related\n",
    "tr_models = []\n",
    "trainers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 4.0 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.040    Total estimated model params size (MB)\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fceb2583f5854352a1d6d7d446c41791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 4.0 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.040    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: effnet_[400, 300]_(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94cdbba305e4b74b38a9e6d7c7679d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 4.0 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.040    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: effnet_[400, 300]_(4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb4a2f6516a44bf9d7c1ec6b6917279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 4.0 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.040    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: effnet_[600, 450]_(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676790fda4ac4e999e15dd831f2a406e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 4.0 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.040    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: effnet_[600, 450]_(4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5617e52628a48f18e58f788397ded60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 4.0 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.040    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: effnet_[800, 600]_(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68daba3549042c492678bc25cee3a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 2.2 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.906     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: effnet_[800, 600]_(4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bfa4860a464c948587fc6838ddb75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 2.2 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.906     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: mobilenet_[400, 300]_(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774bf4a4ad81400886ea6b83da13367b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 2.2 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.906     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: mobilenet_[400, 300]_(4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6053820186de4f138de18b492f2ce6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 2.2 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.906     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: mobilenet_[600, 450]_(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cc70db84e346e38a3520d8dd7952f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 2.2 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.906     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: mobilenet_[600, 450]_(4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc76df27bad44c8a29be7dc639555a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 2.2 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.906     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: mobilenet_[800, 600]_(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136b39a71a0346bb9c373d29122fc99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 1.3 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.023     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: mobilenet_[800, 600]_(4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab6b33a8495484e83f879f40c6ebc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 1.3 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.023     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: shufflenet_[400, 300]_(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ac6337a70f4c5bb815642ca5c17edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 1.3 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.023     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: shufflenet_[400, 300]_(4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3016f12bf906426dbf62bbe97f8f0dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 1.3 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.023     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: shufflenet_[600, 450]_(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649521cc6c0c42f2acfc8f63685ed53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 1.3 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.023     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: shufflenet_[600, 450]_(4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a6d8e07dc5416586d73cb207722b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 1.3 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.023     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: shufflenet_[800, 600]_(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ardhanidzaky/Documents/02 University/02d Thesis/code/skripsi_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed56076c1a164f6b91bcc1505edd6525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for: shufflenet_[800, 600]_(4, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for mod in mod_strings:\n",
    "    for resol in resolutions:\n",
    "        data_transform = return_data_transform(tuple(resol))\n",
    "        \n",
    "        train_dataset = CustomDataset(\n",
    "            dataframe=train, root=ROOT, transform=data_transform\n",
    "            , target_width=resol[1], target_height=resol[0]\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "        \n",
    "        val_dataset = CustomDataset(\n",
    "            dataframe=val, root=ROOT, transform=data_transform\n",
    "            , target_width=resol[1], target_height=resol[0]\n",
    "        )\n",
    "        val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "        for kernel in kernel_size:\n",
    "            # Train the model\n",
    "            if kernel == (4, 3): \n",
    "                _temp_model = FKGLightningModuleV1(2, mod, model_dict, kernel_size=(4,3), change_kernel_size=True)\n",
    "            else: \n",
    "                _temp_model = FKGLightningModuleV1(2, mod, model_dict)\n",
    "\n",
    "            _temp_trainer = pl.Trainer(max_epochs=max_epoch_used, accelerator=\"gpu\", devices=\"auto\")\n",
    "            _temp_trainer.fit(model=_temp_model, train_dataloaders=train_loader)\n",
    "\n",
    "            df_model.append(mod)\n",
    "            df_resolutions.append(resol)\n",
    "            df_kernel.append(kernel)\n",
    "\n",
    "            tr_models.append(_temp_model)\n",
    "            trainers.append(_temp_trainer)\n",
    "\n",
    "            # Test result\n",
    "            _temp_model_to_test = _temp_model\n",
    "            predicted_labels = []\n",
    "            true_labels = []\n",
    "        \n",
    "            _temp_model_to_test.eval()\n",
    "            _temp_model_to_test.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = _temp_model_to_test(inputs)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    \n",
    "                    predicted_labels.extend(predicted.cpu().numpy())\n",
    "                    true_labels.extend(labels.cpu().numpy())\n",
    "                    \n",
    "            rep = classification_report(true_labels, predicted_labels, zero_division=0, output_dict=True)\n",
    "            df_f1.append(rep['weighted avg']['f1-score'])\n",
    "            df_recall.append(rep['weighted avg']['recall'])\n",
    "            df_precision.append(rep['weighted avg']['precision'])\n",
    "            df_accuracy.append(rep['accuracy'])\n",
    "\n",
    "            # Get training loss\n",
    "            plt.plot([tensor.item() for tensor in _temp_model.training_loss], label='Training Loss')\n",
    "            plt.title(f'{mod}_{resol}_{kernel}')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.savefig(f'{mod}_{resol}_{kernel}')\n",
    "            plt.clf()\n",
    "\n",
    "            # Save the model\n",
    "            torch.save(_temp_model.model.state_dict(), f'{mod}_{resol}_{kernel}.pth')\n",
    "            print(f'Done for: {mod}_{resol}_{kernel}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start: 22.37 WIB\n",
    "# End: 23.16 WIB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = []\n",
    "df_resolutions = []\n",
    "df_kernel = []\n",
    "\n",
    "for mod in ['effnet', 'mobilenet', 'shufflenet']:\n",
    "    for resol in [[300, 400], [450, 600], [600, 800]]:\n",
    "        for kernel in [(3,3), (4,3)]:\n",
    "            df_model.append(mod)\n",
    "            df_resolutions.append(resol)\n",
    "            df_kernel.append(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame({\n",
    "    'Model': df_model\n",
    "    , 'Resolutions': df_resolutions\n",
    "    , 'Kernel Size': df_kernel\n",
    "    , 'F1-score': df_f1\n",
    "    , 'Recall': df_recall\n",
    "    , 'Precision': df_precision\n",
    "    , 'Accuracy': df_accuracy\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Resolutions</th>\n",
       "      <th>Kernel Size</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>effnet</td>\n",
       "      <td>[300, 400]</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>0.516484</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>effnet</td>\n",
       "      <td>[300, 400]</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>0.321212</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effnet</td>\n",
       "      <td>[450, 600]</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.552381</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>effnet</td>\n",
       "      <td>[450, 600]</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>effnet</td>\n",
       "      <td>[600, 800]</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>effnet</td>\n",
       "      <td>[600, 800]</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mobilenet</td>\n",
       "      <td>[300, 400]</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>0.709890</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mobilenet</td>\n",
       "      <td>[300, 400]</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>0.474747</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mobilenet</td>\n",
       "      <td>[450, 600]</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mobilenet</td>\n",
       "      <td>[450, 600]</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mobilenet</td>\n",
       "      <td>[600, 800]</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mobilenet</td>\n",
       "      <td>[600, 800]</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>shufflenet</td>\n",
       "      <td>[300, 400]</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>shufflenet</td>\n",
       "      <td>[300, 400]</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>shufflenet</td>\n",
       "      <td>[450, 600]</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>shufflenet</td>\n",
       "      <td>[450, 600]</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>shufflenet</td>\n",
       "      <td>[600, 800]</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>shufflenet</td>\n",
       "      <td>[600, 800]</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>0.516484</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model Resolutions Kernel Size  F1-score  Recall  Precision  Accuracy\n",
       "0       effnet  [300, 400]      (3, 3)  0.516484     0.5   0.541667       0.5\n",
       "1       effnet  [300, 400]      (4, 3)  0.321212     0.3   0.400000       0.3\n",
       "2       effnet  [450, 600]      (3, 3)  0.400000     0.4   0.552381       0.4\n",
       "3       effnet  [450, 600]      (4, 3)  0.576471     0.7   0.490000       0.7\n",
       "4       effnet  [600, 800]      (3, 3)  0.600000     0.6   0.600000       0.6\n",
       "5       effnet  [600, 800]      (4, 3)  0.576471     0.7   0.490000       0.7\n",
       "6    mobilenet  [300, 400]      (3, 3)  0.709890     0.7   0.733333       0.7\n",
       "7    mobilenet  [300, 400]      (4, 3)  0.474747     0.5   0.812500       0.5\n",
       "8    mobilenet  [450, 600]      (3, 3)  0.680000     0.7   0.675000       0.7\n",
       "9    mobilenet  [450, 600]      (4, 3)  0.762500     0.8   0.844444       0.8\n",
       "10   mobilenet  [600, 800]      (3, 3)  0.576471     0.7   0.490000       0.7\n",
       "11   mobilenet  [600, 800]      (4, 3)  0.576471     0.7   0.490000       0.7\n",
       "12  shufflenet  [300, 400]      (3, 3)  0.800000     0.8   0.800000       0.8\n",
       "13  shufflenet  [300, 400]      (4, 3)  0.466667     0.5   0.437500       0.5\n",
       "14  shufflenet  [450, 600]      (3, 3)  0.680000     0.7   0.675000       0.7\n",
       "15  shufflenet  [450, 600]      (4, 3)  0.466667     0.5   0.437500       0.5\n",
       "16  shufflenet  [600, 800]      (3, 3)  0.680000     0.7   0.675000       0.7\n",
       "17  shufflenet  [600, 800]      (4, 3)  0.516484     0.5   0.541667       0.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNI8a+vbPZCOX45t1mhHzTr",
   "gpuType": "T4",
   "mount_file_id": "1n1V1zBC3VVdHMWrc_BtrOlW7jURDYWVe",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "skripsi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
